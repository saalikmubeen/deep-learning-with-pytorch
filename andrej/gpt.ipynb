{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d47b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ab2781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 500 characters\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75fef6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "# print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "\n",
    "# We can tradeoff between the size of the vocabulary(code book size) and the length of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b607ee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Tokenization: convert characters to integers and vice versa\n",
    "# Character level language model\n",
    "\n",
    "# Example tokenization methods: SentencePiece, Byte Pair Encoding (BPE), WordPiece\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)} # character:integer mapping\n",
    "itos = {i: ch for i, ch in enumerate(chars)} # integer:character mapping\n",
    "# encoder: take a string, output a list of integers\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "# decoder: take a list of integers, output a string\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2194161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# 1115394 is total number of characters in the dataset\n",
    "print(data.shape, data.dtype)\n",
    "# the 200 characters we looked at earier will to the GPT look like this\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4b8ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "# block size or context length is the length of the input sequence, how many tokens\n",
    "# we feed to the model at once\n",
    "block_size = 8\n",
    "\n",
    "print(train_data[:block_size+1]) # tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
    "\n",
    "# These are the first 9 characters in the training data. When we sample chunk of data like this,\n",
    "# this has multiple examples or samples packed into it because all these characters follow each other.\n",
    "# And when we plug it into the transformer, we are going to simultaneously train it to make prediction\n",
    "# at one of these positions.\n",
    "# We can think of this as a sliding window over the data and sample the next character as the target.\n",
    "# So in the chunk of 9 characters, there are 8 individual examples in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8369e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([18, 47, 56, 57, 58,  1, 15, 47]) \n",
      "y: tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1] # Offset by one, the target is the next character in the sequence\n",
    "\n",
    "print(f\"x: {x} \\ny: {y}\")\n",
    "\n",
    "\n",
    "\n",
    "# These are the 8 examples hidden in the chunk of 9 characters that we sampled from the training set.\n",
    "# We train on all the examples here with context between 1 all the way upto the context of block_size i.e. 8.\n",
    "# t can be thought of as the time dimension, the position in the sequence or the length of the input sequence.\n",
    "# t can go from 0 to block_size-1.\n",
    "for t in range(block_size):\n",
    "    # print(t) [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6feb2e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---------------------------------------------------\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independent sequences or examples will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Generate 4(batch_size) random integers(starting indices) between 0 and len(data) - block_size.\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))   # shape (batch_size,)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)  # (batch_size, block_size)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)  # (batch_size, block_size)\n",
    "\n",
    "    # sequences_x = []\n",
    "    # sequences_y = []\n",
    "    # for i in ix:\n",
    "    #     seq_x = data[i:i+block_size+1]\n",
    "    #     sequences_x.append(seq_x)\n",
    "\n",
    "    #     seq_y = data[i+1:i+block_size+1]\n",
    "    #     sequences_y.append(seq_y)\n",
    "    # x = torch.stack(sequences_x, dim=0) # (batch_size, block_size+1)\n",
    "    # y = torch.stack(sequences_y, dim=0) # (batch_size, block_size)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('---------------------------------------------------')\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d49bdb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)  # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xb)  # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02e4e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.4649, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([1, 101])\n",
      "\n",
      "SmVddomOVWydk3'BrlK\n",
      "QduWGGHPmiu&UXBlIHTZ'yfsDuEtqWPUlOZt&-lV&qBohwN.l;N3z:miimwvg,gAo3EPN3hOw$!VyTuE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "embedding_size = 32  # each character is represented by a vector of size embedding_size\n",
    "block_size = 8  # context length, how many characters we feed to the model at once\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size=embedding_size):\n",
    "        super().__init__()\n",
    "        # Each character is represented by a vector of size embedding_size.\n",
    "        # So in our case we have a vocabulary of size vocab_size = 65 and each character\n",
    "        # is represented by a vector of size embedding_size = 32.\n",
    "        # In this case, the token embedding table is a square matrix of size (vocab_size, embedding_size).\n",
    "        # Each row of this matrix corresponds to a character in the vocabulary with its embedding vector.\n",
    "\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # each position from 0 to block_size-1 will also have a corresponding embedding vector\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, embedding_size)  # positional embeddings\n",
    "\n",
    "        self.lm_head = nn.Linear(embedding_size, vocab_size)  # output layer to map embeddings to logits\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx is of shape (B, T) where B is the batch size and T is the block size, here (4, 8)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # So each of the 8 characters (integer correspoding to that character) for every example\n",
    "        # will index into the token embedding table to get their corresponding embedding vectors.\n",
    "        # So every single integer in our input sequence is going to refer to the embedding table\n",
    "        # and is going to pluck out a row from the embedding table corresponding to that integer\n",
    "        # i.e it's index and every integer will be replaced by its corresponding embedding vector.\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_embeddings = self.token_embedding_table(idx)  # (B,T,C) = (4, 8, 32)\n",
    "\n",
    "        # All the integers from 0 to block_size-1 or T - 1 will get embedded through the\n",
    "        # positional embedding table to get and pluck out a row or embedding vector\n",
    "        # corresponding to that integer.\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T))  # (T, C) = (8, 32)\n",
    "\n",
    "        # now x holds not just the token identities but the positions at which the tokens occur\n",
    "        x = token_embeddings + positional_embeddings  # (B, T, C) = (4, 8, 32)\n",
    "        # x = token_embeddings\n",
    "\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size) = (4, 8, 65)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, vocab_size  = logits.shape\n",
    "            logits = logits.view(B*T, vocab_size) # (32, 65)\n",
    "            targets = targets.view(B*T)  # (32,)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) # logits is (B, T, C)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# kick off generation with character corresponding to 0 which is a 'new line character'\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # 1 batch, 1 context length\n",
    "generated = m.generate(idx=idx, max_new_tokens=100)\n",
    "print(generated.shape) # [1, 101] 1 for batch, we generate until context length becomes 100, 101 tokens (1 initial + 100 generated)\n",
    "print(decode(generated[0].tolist()))  # decode the generated sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84508080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.569643020629883\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(1000):  # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29c6d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IAthape f gidom:\n",
      "n, t nrd v t:\n",
      "Q oto t hieret wouan r chersod  sesthande pKs s the s bleaunthisid EN\n",
      "WES:\n",
      "Te hee woce ld g wot.\n",
      "An damedrpl ph an\n",
      "AKe hameg thy cavind trkelyovesyave\n",
      "TAKoude uth veillaBartga;\n",
      "$lenu inonofete mrave\n",
      "\n",
      "Mvatlo mis'ray\n",
      "H t?\n",
      "\n",
      "Cpe wirs watheanthinerense athandst.\n",
      "AO-pit chesellongoEx theroidlos tery I uc ithe methene,a hd de Wear at yt be hite phanim mpasig' an.\n",
      "\n",
      "rathan?\n",
      " llloc\n",
      "\n",
      "ASesipp wilele ba eyo t nll tond pthealders as pde llaldsthed,\n",
      "AE lei're f ML\n",
      "Cxhalis ton y b\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bac2ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e7975",
   "metadata": {},
   "source": [
    "###  The Mathematical Trick of Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c92190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 7])\n",
      "tensor([[9., 9., 3., 9., 8., 4., 8.],\n",
      "        [3., 5., 4., 3., 3., 8., 9.],\n",
      "        [1., 6., 8., 6., 0., 0., 3.],\n",
      "        [4., 3., 8., 6., 6., 1., 7.],\n",
      "        [6., 8., 6., 5., 2., 4., 4.]])\n",
      "torch.Size([4, 5, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9.0000, 9.0000, 3.0000, 9.0000, 8.0000, 4.0000, 8.0000],\n",
       "        [6.0000, 7.0000, 3.5000, 6.0000, 5.5000, 6.0000, 8.5000],\n",
       "        [4.3333, 6.6667, 5.0000, 6.0000, 3.6667, 4.0000, 6.6667],\n",
       "        [4.2500, 5.7500, 5.7500, 6.0000, 4.2500, 3.2500, 6.7500],\n",
       "        [4.6000, 6.2000, 5.8000, 5.8000, 3.8000, 3.4000, 6.2000]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 5, 7 # batch, time, channels\n",
    "\n",
    "x = torch.randint(10, (B, T, C)).float()\n",
    "# x = torch.ones((B, T, C))\n",
    "\n",
    "# We want to compute the attention scores(for now just the average) for each token in the sequence\n",
    "# with respect to every other token preceeding it and not the ones that come after it.\n",
    "# We can do this by taking the average of the token embeddings for each token in the sequence\n",
    "# upto the current token and not the ones that come after it.\n",
    "# This is a very simple example of self attention.\n",
    "# So say for token 5 in sequence of 8 tokens, we want to average the embeddings of tokens\n",
    "# i,e channels of 0, 1, 2, 3, 4 and 5 and that will be the attention score for token 5.\n",
    "print(x.shape)\n",
    "print(x[1])\n",
    "\n",
    "\n",
    "# Version 1\n",
    "scores = torch.zeros((B, T, C))  # initialize scores tensor to hold the attention scores\n",
    "for b in range(B):  # batch dimension\n",
    "    for t in range(T):  # time dimension\n",
    "\n",
    "        # shape (t, C), t will go from 0 to T-1 i.e 0 to 4 in this case, (1, 7) (2, 7) (3, 7) (4, 7) (5, 7)\n",
    "        prev = x[b, :t+1, :]  # take all the tokens upto and including the current token\n",
    "        attention_score = torch.mean(prev, dim=0)  # average the embeddings across the time dimension | shape (7,)\n",
    "        # print(attention_score.shape) # (C,) # (7,)\n",
    "        scores[b, t] = attention_score  # store the attention score in the scores tensor\n",
    "\n",
    "print(scores.shape)  # (B, T, C) = (4, 5, 7)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce745e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "torch.Size([4, 5, 7])\n",
      "tensor([[9., 9., 3., 9., 8., 4., 8.],\n",
      "        [3., 5., 4., 3., 3., 8., 9.],\n",
      "        [1., 6., 8., 6., 0., 0., 3.],\n",
      "        [4., 3., 8., 6., 6., 1., 7.],\n",
      "        [6., 8., 6., 5., 2., 4., 4.]])\n",
      "tensor([[ 9.,  9.,  3.,  9.,  8.,  4.,  8.],\n",
      "        [12., 14.,  7., 12., 11., 12., 17.],\n",
      "        [13., 20., 15., 18., 11., 12., 20.],\n",
      "        [17., 23., 23., 24., 17., 13., 27.],\n",
      "        [23., 31., 29., 29., 19., 17., 31.]])\n",
      "tensor([[9.0000, 9.0000, 3.0000, 9.0000, 8.0000, 4.0000, 8.0000],\n",
      "        [6.0000, 7.0000, 3.5000, 6.0000, 5.5000, 6.0000, 8.5000],\n",
      "        [4.3333, 6.6667, 5.0000, 6.0000, 3.6667, 4.0000, 6.6667],\n",
      "        [4.2500, 5.7500, 5.7500, 6.0000, 4.2500, 3.2500, 6.7500],\n",
      "        [4.6000, 6.2000, 5.8000, 5.8000, 3.8000, 3.4000, 6.2000]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[9.0000, 9.0000, 3.0000, 9.0000, 8.0000, 4.0000, 8.0000],\n",
      "        [6.0000, 7.0000, 3.5000, 6.0000, 5.5000, 6.0000, 8.5000],\n",
      "        [4.3333, 6.6667, 5.0000, 6.0000, 3.6667, 4.0000, 6.6667],\n",
      "        [4.2500, 5.7500, 5.7500, 6.0000, 4.2500, 3.2500, 6.7500],\n",
      "        [4.6000, 6.2000, 5.8000, 5.8000, 3.8000, 3.4000, 6.2000]])\n"
     ]
    }
   ],
   "source": [
    "# Version 2\n",
    "\n",
    "# Doing the same thing with Matrix Multiplication\n",
    "# We can do the same thing with matrix multiplication.\n",
    "# We can take the average of all the tokens upto and including the current token\n",
    "# by multiplying the token embeddings with a triangular matrix of ones.\n",
    "# This triangular matrix will have ones in the upper triangle and zeros in the lower triangle.\n",
    "# This will ensure that we only take the tokens upto and including the current token\n",
    "triangular_matrix = torch.tril(torch.ones((T, T)))  # shape (T, T) = (5, 5)\n",
    "print(triangular_matrix)\n",
    "scores = triangular_matrix @ x\n",
    "print(scores.shape)  # (B, T, C) = (4, 5, 7)\n",
    "print(x[1])\n",
    "print(scores[1])  # this gives us the sum of all the tokens upto and including the current token\n",
    "# But we want the average, so we divide by the number of tokens\n",
    "\n",
    "# print(torch.arange(1, T+1)) # tensor([1, 2, 3, 4, 5])\n",
    "scores = scores / torch.arange(1, T+1).view(1, T, 1)  # shape (1, T, 1) = (1, 5, 1)\n",
    "print(scores[1])  # this gives us the average of all the tokens upto and including the current token\n",
    "\n",
    "# OR\n",
    "triangular_matrix = triangular_matrix / triangular_matrix.sum(dim=1, keepdim=True)  # normalize the triangular matrix\n",
    "print(triangular_matrix) # each row sums to 1\n",
    "scores = triangular_matrix @ x  # now this is the average of all the tokens upto and including the current token\n",
    "# (T, T) @ (B, T, C) = (B, T, C)\n",
    "print(scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b92d1984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[9., 9., 3., 9., 8., 4., 8.],\n",
      "        [3., 5., 4., 3., 3., 8., 9.],\n",
      "        [1., 6., 8., 6., 0., 0., 3.],\n",
      "        [4., 3., 8., 6., 6., 1., 7.],\n",
      "        [6., 8., 6., 5., 2., 4., 4.]])\n",
      "tensor([[9.0000, 9.0000, 3.0000, 9.0000, 8.0000, 4.0000, 8.0000],\n",
      "        [6.0000, 7.0000, 3.5000, 6.0000, 5.5000, 6.0000, 8.5000],\n",
      "        [4.3333, 6.6667, 5.0000, 6.0000, 3.6667, 4.0000, 6.6667],\n",
      "        [4.2500, 5.7500, 5.7500, 6.0000, 4.2500, 3.2500, 6.7500],\n",
      "        [4.6000, 6.2000, 5.8000, 5.8000, 3.8000, 3.4000, 6.2000]])\n"
     ]
    }
   ],
   "source": [
    "# Version 3\n",
    "\n",
    "# Using softmax to compute the attention scores\n",
    "# We can also use softmax to compute the attention scores.\n",
    "\n",
    "triangular_matrix = torch.tril(torch.ones((T, T)))  # shape (T, T) = (5, 5)\n",
    "wei = torch.zeros((T, T))  # initialize weights tensor to hold the attention scores\n",
    "\n",
    "# In the wei tensor, we will set the upper triangle to -inf. The places where we have 0 in the\n",
    "# triangular matrix, we will set the corresponding positions in the wei tensor to -inf.\n",
    "#  So that when we apply softmax, the softmax will ignore those positions and assign 0 probability to them.\n",
    "wei = wei.masked_fill(triangular_matrix == 0, float('-inf'))  # set the upper triangle to -inf\n",
    "print(wei)\n",
    "\n",
    "wei = F.softmax(wei, dim=-1)  # apply softmax to get the attention scores\n",
    "print(wei)\n",
    "\n",
    "scores = wei @ x  # (T, T) @ (B, T, C) = (B, T, C)\n",
    "print(x[1])\n",
    "print(scores[1])  # this gives us the average of all the tokens upto and including the current token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "077c39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -0.5330,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,\n",
      "             -inf],\n",
      "        [-14.1926,  -9.7073,     -inf,     -inf,     -inf,     -inf,     -inf,\n",
      "             -inf],\n",
      "        [  6.8117,   1.9836,   9.7450,     -inf,     -inf,     -inf,     -inf,\n",
      "             -inf],\n",
      "        [ -3.5694,  -6.2294,  -6.2758,  -6.3100,     -inf,     -inf,     -inf,\n",
      "             -inf],\n",
      "        [  6.8332,   9.4518,   8.6119,   8.3535,   8.5837,     -inf,     -inf,\n",
      "             -inf],\n",
      "        [  3.4085,  -2.4052,  -0.9684,  -3.2428,   0.2396,  -0.5170,     -inf,\n",
      "             -inf],\n",
      "        [ -0.5364,  -4.5374,   0.6402,  -4.6938,  -5.0907,   0.4215, -10.4793,\n",
      "             -inf],\n",
      "        [ -1.0625,   3.4254,   7.2073,   1.1701,   3.9532,   4.1309,   4.3983,\n",
      "           2.1042]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Single Head Self Attention\n",
    "\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "\n",
    "x = torch.randint(10, (B, T, C)).float() # (4, 8, 32)\n",
    "\n",
    "# Each 32 dimensional token emits 3 vectors K, Q, V\n",
    "# K is the key vector, Q is the query vector and V is the value vector.\n",
    "\n",
    "\n",
    "# You can kind of think as K, Q, V as projection the input token embedding which initially\n",
    "# is of size C(32) or in 32 dimensional space and projecting it to a lower dimensional space like 16.\n",
    "key = nn.Linear(C, 16, bias=False)  # key projection\n",
    "query = nn.Linear(C, 16, bias=False)  # query projection\n",
    "value = nn.Linear(C, 16, bias=False)  # value projection\n",
    "\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "v = value(x)  # (B, T, 16)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "\n",
    "\n",
    "# Scaled Dot Product Attention\n",
    "# To prevent the dot products from growing too large, we scale them by the square root of the key dimension.\n",
    "# This helps to stabilize the gradients during training.\n",
    "scale = 16 ** 0.5\n",
    "wei = wei / scale\n",
    "\n",
    "# Kinda forget about B dimension for now, we will just focus on T dimension.\n",
    "# In Q, each row is a query vector for a token with 16 dimensions.\n",
    "# In K_transpose, each column is a key vector with 16 dimensions for a token.\n",
    "# In V, each row is a value vector for a token with 16 dimensions.\n",
    "\n",
    "# So when we do q @ k.transpose(-2, -1), we are computing the dot product between\n",
    "# each query vector and each key vector for every token in the sequence.\n",
    "# So the first row of wei of shape(T, T) will be the dot product of the query vector of the first token\n",
    "# with all the key vectors of all the tokens in the sequence. So the first row of wei will\n",
    "# contain the attention scores for the first token with respect to all the tokens in the sequence\n",
    "# and tell us how much attention the first token should pay to all the tokens in the sequence.\n",
    "# Similarly the second row tells us how much attention the second token should pay to all the\n",
    "# tokens in the sequence.\n",
    "# And this happens for all the examples in the batch simultaneously.\n",
    "\n",
    "\n",
    "# Causal self attention is a mechanism that allows each token in the sequence to attend to all the tokens\n",
    "# that come before it and not the ones that come after it.\n",
    "# So we want to set the upper triangle of the wei tensor to -inf so that when we apply softmax,\n",
    "# the softmax will ignore those positions and assign 0 weights to them.\n",
    "tril = torch.tril(torch.ones((T, T)))  # shape (T, T) = (8, 8)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # set the upper triangle to -inf\n",
    "print(wei[0])\n",
    "\n",
    "\n",
    "# And now what we do is take this wei tensor and apply softmax to it.\n",
    "wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "\n",
    "\n",
    "out = wei @ v # (B, T, T) @ (B, T, 16) = (B, T, 16)\n",
    "# Now we have the attention scores for each token in the sequence with respect to all the tokens in the sequence.\n",
    "# Now we can use these attention scores to compute the output of the self attention layer.\n",
    "# Say for the first token, the first row of wei tells us how much attention first token has to pay to\n",
    "# each token in the sequence. We take each score in the row multiply it with the corresponding value vector\n",
    "# for which the score was computed and add them up to get the output for the first token.\n",
    "# This will be the output of the self attention layer for the first token.\n",
    "# Say scores in the first row of wei are [0.1, 0.2, 0.3, 0.4, 0.0, 0.0, 0.0, 0.0]\n",
    "# And the value vectors for the tokens are: v1, v2, v3, v4, v5, v6, v7, v8 (each is a vector of size 16)\n",
    "# So the output for the first token will be:\n",
    "# output = 0.1 * v1 + 0.2 * v2 + 0.3 * v3 + 0.4 * v4 + 0.0 * v5 + 0.0 * v6 + 0.0 * v7 + 0.0 * v8\n",
    "# This will be the output of the self attention layer for the first token and the first row of out will be\n",
    "# the output and the enhanced representation of the first token.\n",
    "# This is exactly what this matrix multiplication does between wei and v does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec83f8c",
   "metadata": {},
   "source": [
    "## Attention Matrix A (4x4)\n",
    "\n",
    "Each row contains attention weights for one token over all tokens (including itself).\n",
    "\n",
    "Let:\n",
    "A = \n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
    "0.3 & 0.3 & 0.2 & 0.2 \\\\\n",
    "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
    "0.4 & 0.3 & 0.2 & 0.1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "## Value Matrix V (4x10)\n",
    "\n",
    "Each row is a value vector of dimension 10:\n",
    "\n",
    "V =\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "## Compute Output Z = A × V (4x10)\n",
    "\n",
    "Let’s compute row 0 of Z explicitly:\n",
    "\n",
    "Z₀ = 0.1 * V₀ + 0.2 * V₁ + 0.3 * V₂ + 0.4 * V₃\n",
    "\n",
    "Break it down:\n",
    "\n",
    "- 0.1 * V₀ = [0.1, 0, 0.1, 0, 0.1, 0, 0.1, 0, 0.1, 0]\n",
    "- 0.2 * V₁ = [0, 0.2, 0, 0.2, 0, 0.2, 0, 0.2, 0, 0.2]\n",
    "- 0.3 * V₂ = [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
    "- 0.4 * V₃ = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "Now sum them up:\n",
    "\n",
    "Z₀ = \n",
    "[\n",
    "\n",
    "0.1 + 0   + 0.3 + 0,  \n",
    "0   + 0.2 + 0.3 + 0,  \n",
    "0.1 + 0   + 0.3 + 0,  \n",
    "0   + 0.2 + 0.3 + 0,  \n",
    "0.1 + 0   + 0.3 + 0,  \n",
    "0   + 0.2 + 0.3 + 0,  \n",
    "0.1 + 0   + 0.3 + 0,  \n",
    "0   + 0.2 + 0.3 + 0,  \n",
    "0.1 + 0   + 0.3 + 0,  \n",
    "0   + 0.2 + 0.3 + 0  \n",
    "\n",
    "]\n",
    "\n",
    "Z₀ =\n",
    "[0.4, 0.5, 0.4, 0.5, 0.4, 0.5, 0.4, 0.5, 0.4, 0.5]\n",
    "\n",
    "Repeat the same for Z₁, Z₂, Z₃ to get the full matrix Z ∈ ℝ⁴ˣ¹⁰.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d4d76dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "# Multi Head Self Attention\n",
    "# In practice, we use multiple heads of self attention to capture different aspects of the input sequence.\n",
    "# And then we concatenate the outputs of all the heads and pass it through a linear layer to get the final output.\n",
    "\n",
    "# Say we have 2 heads of self attention, we will have 2 sets of K, Q, V vectors.\n",
    "num_heads = 2\n",
    "d_model = 32  # dimension of the input token embeddings\n",
    "d_k = d_model // num_heads  # dimension of each head, here 16\n",
    "\n",
    "B, T, C = 4, 8, d_model  # batch, time, channels\n",
    "\n",
    "x = torch.randint(10, (B, T, C)).float()  # (4, 8, 32)\n",
    "\n",
    "# Each head will have its own set of linear layers for K, Q, V\n",
    "key_1 = nn.Linear(C, d_k, bias=False)    # key projection for head 1\n",
    "query_1 = nn.Linear(C, d_k, bias=False)  # query projection for head 1\n",
    "value_1 = nn.Linear(C, d_k, bias=False)  # value projection for head 1\n",
    "\n",
    "k_1 = key_1(x)  # (B, T, 16)\n",
    "q_1 = query_1(x)  # (B, T, 16)\n",
    "v_1 = value_1(x)  # (B, T, 16)\n",
    "wei_1 = q_1 @ k_1.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "# Scaled Dot Product Attention for head 1\n",
    "scale = d_k ** 0.5\n",
    "wei_1 = wei_1 / scale  # (B, T, T)\n",
    "# Causal self attention for head 1\n",
    "mask_1 = torch.tril(torch.ones(T, T), diagonal=1).bool()  # upper triangular matrix\n",
    "wei_1 = wei_1.masked_fill(mask_1, float('-inf'))  # apply mask\n",
    "wei_1 = wei_1.softmax(dim=-1)  # (B, T, T)\n",
    "# Now we can compute the output for head 1\n",
    "out_1 = wei_1 @ v_1  # (B, T, T) @ (B, T, 16) = (B, T, 16)\n",
    "\n",
    "\n",
    "# Now we do the same for head 2\n",
    "\n",
    "key_2 = nn.Linear(C, d_k, bias=False)    # key projection for head 2\n",
    "query_2 = nn.Linear(C, d_k, bias=False)  # query projection for head 2\n",
    "value_2 = nn.Linear(C, d_k, bias=False)  # value projection for head 2\n",
    "\n",
    "k_2 = key_2(x)  # (B, T, 16)\n",
    "q_2 = query_2(x)  # (B, T, 16)\n",
    "v_2 = value_2(x)  # (B, T, 16)\n",
    "\n",
    "wei_2 = q_2 @ k_2.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "# Scaled Dot Product Attention for head 2\n",
    "scale = d_k ** 0.5\n",
    "wei_2 = wei_2 / scale  # (B, T, T)\n",
    "# Causal self attention for head 2\n",
    "mask_2 = torch.tril(torch.ones(T, T), diagonal=1).bool()  # upper triangular matrix\n",
    "wei_2 = wei_2.masked_fill(mask_2, float('-inf'))  # apply mask\n",
    "wei_2 = wei_2.softmax(dim=-1)  # (B, T, T)\n",
    "# Now we can compute the output for head 2\n",
    "out_2 = wei_2 @ v_2  # (B, T, T) @ (B, T, 16) = (B, T, 16)\n",
    "\n",
    "print(out_1.shape)  # (4, 8, 16)\n",
    "print(out_2.shape)  # (4, 8, 16)\n",
    "\n",
    "# Now we concatenate the outputs of both heads\n",
    "out = torch.cat((out_1, out_2), dim=-1)  # (B, T, 16*2) = (4, 8, 32)\n",
    "# And we pass it through a linear layer to get the final output\n",
    "\n",
    "# Each token in the sequence of size 32 is separately passsed through a linear layer\n",
    "lm_head = nn.Linear(d_model, d_model)  # output layer to map embeddings to logits\n",
    "final_output = lm_head(out)  # (B, T, C) = (4, 8, 32)\n",
    "print(final_output.shape)  # (4, 8, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde42ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = mps\n",
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 500: train loss 2.3137, val loss 2.3145\n",
      "step 1000: train loss 2.1030, val loss 2.1295\n",
      "step 1500: train loss 1.9671, val loss 2.0329\n",
      "step 2000: train loss 1.8799, val loss 1.9676\n",
      "step 2500: train loss 1.8212, val loss 1.9456\n",
      "step 3000: train loss 1.7750, val loss 1.9170\n",
      "step 3500: train loss 1.7498, val loss 1.8924\n",
      "step 4000: train loss 1.7191, val loss 1.8581\n",
      "step 4500: train loss 1.6964, val loss 1.8502\n",
      "step 4999: train loss 1.6633, val loss 1.8268\n",
      "\n",
      "Forsul presenchelders, else dofenievey:\n",
      "All know I heaver the horselted, my charn blones,\n",
      "Beath Lord, as a simpery imbuag hold\n",
      "by serveing shalk, as leht thy\n",
      "who croes fears it of sulb.\n",
      "\n",
      "HENRY BOLINGBRARET:\n",
      "Say live, I mosty frishond not slead 'tis proging.\n",
      "\n",
      "QUEEN MARGAn:\n",
      "Why, they foull lead,\n",
      "I agailn art your had cried men\n",
      "Is thoughmy that prongmanoroud Volnow,\n",
      "And say shall rachaly all was had;\n",
      "Thought from shap thy, as it sirring a tasts latesse my doed our challoyamen,\n",
      "Kathing a eye nothor hand fear fight huspily\n",
      "Wetcion or ord life, by say and of\n",
      "Hell-husar no his parerson: the ignorount, the waill clister that yorrows,\n",
      "As we ever this aglavant, for him agless gaverton't as gawn arth man, broth old,\n",
      "Bothohble revents letters not, areetted bid my vericy had wolk.\n",
      "\n",
      "COMILINES:\n",
      "Then up his boslents againg it she evousan:\n",
      "In trow yourself!\n",
      "\n",
      "GLOUCESTER:\n",
      "Where, throw is by thy was own was taling\n",
      "Of gintre our bresence reach us soung dair thee.\n",
      "\n",
      "VOLUS:\n",
      "It was ashy flace, shame, gook!\n",
      "Were fort, that our husband, well see, them whereforomm a look.\n",
      "And this that our fear us. By clonses judget loud one his him;\n",
      "When whow 'tiss all hower is rewning\n",
      "Ond me in the father them spigntrets him sound, to thou princely morry,-ams my gace\n",
      "That's naccuse, minent all hence\n",
      "aid the lady erry veain\n",
      "To thought thee faindelys\n",
      "We it knowh the diart. 'Tistater anjy that the diep ingence a friend, for thereing a gricrefarmfice are with take man'd that dexknowled,\n",
      "The seleans--slordome art at, miscourt us:\n",
      "sill who flords, my devourst numbseed and evernes asider:\n",
      "\n",
      "HENRY BOL:\n",
      "Int, what! part you sleal, thoug,\n",
      "Whell admader, how, my way bread plosant soul mil, if you in onclingtand: an\n",
      "Without on, slaves both our than widdo.\n",
      "\n",
      "MENENIUS:\n",
      "Is have evensely at I rapposed,\n",
      "geive it that me the cousanting and mathale I scrued ocquor:\n",
      "But thee, civoole is his mine, he past:\n",
      "Why baring you day, fall the issiver me.\n",
      "\n",
      "NOLUS:\n",
      "I thoou him barrys is say has own his what father honour.\n",
      "\n",
      "WARWICK:\n",
      "And Or for\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # 64   # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3 # 10-4\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64 # 384\n",
    "# d_model = n_embd  # dimension of the model, same as the embedding size\n",
    "# d_k = 64/4 = 16  # dimension of each head, here 16\n",
    "n_head = 4\n",
    "n_layer = 4 # 6\n",
    "dropout = 0.0 # 0.2\n",
    "# ------------\n",
    "\n",
    "# send the network to the GPU if available\n",
    "device = 'mps' if torch.backends.mps.is_available() else (\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device = \" + device)\n",
    "if device == 'cpu':\n",
    "    print(\"WARNING: Using CPU will cause slower train times\")\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "# encoder: take a string, output a list of integers\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "# decoder: take a list of integers, output a string\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # block_size is the maximum context length\n",
    "        # this is not the parameter of the module\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(\n",
    "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    # This is on a per token basis, so it does not have any parameters that depend on the sequence length.\n",
    "    # It is applied to each token independently. So each token independently goes through the\n",
    "    # feedforward network.\n",
    "\n",
    "    # ? WE PRODUCE THE PROBABILITY FOR EVERY SINGLE TOKEN THAT MIGHT COME NEXT\n",
    "    # ? AND WE DO THIS AT EVERY POINT IN TIME OF THAT TRANSFORMER  i.e FOR EVERY SINGLE\n",
    "    # ? TOKEN IN THE SEQUENCE AND WE DO THIS IN PARALLEL FOR ALL THE TOKENS IN THE SEQUENCE\n",
    "    # ? SO WE HAVE A SEPARATE FEEDFORWARD NETWORK FOR EVERY SINGLE TOKEN IN THE SEQUENCE\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # this is where tokens attend and communicate with each other\n",
    "        self.ffwd = FeedFoward(n_embd)  # this is where tokens think individually about the information they gathered in attention step.\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connection around self-attention and feedforward\n",
    "\n",
    "        # pre-norm formulation\n",
    "        norm_x1 = self.ln1(x)      # layer norm before self-attention\n",
    "        x = x + self.sa(norm_x1)   # residual connection around self-attention\n",
    "\n",
    "        norm_x2 = self.ln2(x)      # layer norm before feedforward\n",
    "        x = x + self.ffwd(norm_x2) # residual connection around feedforward\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # ? WE PRODUCE THE PROBABILITY FOR EVERY SINGLE TOKEN THAT MIGHT COME NEXT\n",
    "        # ? AND WE DO THIS AT EVERY POINT IN TIME OF THAT TRANSFORMER  i.e FOR EVERY SINGLE\n",
    "        # ? TOKEN IN THE SEQUENCE AND WE DO THIS IN PARALLEL FOR ALL THE TOKENS IN THE SEQUENCE\n",
    "        # ? SO WE HAVE A SEPARATE FEEDFORWARD NETWORK FOR EVERY SINGLE TOKEN IN THE SEQUENCE.\n",
    "\n",
    "        # ? SO THIS lm_head IS A LINEAR LAYER THAT MAPS THE EMBEDDING OF EACH TOKEN\n",
    "        # ? TO THE LOGITS FOR THE NEXT TOKEN IN THE SEQUENCE AND THIS IS DONE IN PARALLEL FOR ALL\n",
    "        # ? THE TOKENS IN THE SEQUENCE AND self.lm_head THUS RUNS ON EVERY SINGLE TOKEN IN THE SEQUENCE\n",
    "        # ? SEPARATELY AND IN PARALLEL.\n",
    "\n",
    "        # ? WE ARE PREDICTING THE NEXT TOKEN FOR EACH TOKEN IN THE SEQUENCE\n",
    "        # ? SO THIS IS A MULTI-HEAD CAUSAL SELF-ATTENTION MECHANISM THAT ALLOWS\n",
    "        # ? EACH TOKEN TO ATTEND TO ALL OTHER TOKENS IN THE SEQUENCE SIMULTANEOUSLY.\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "\n",
    "        # ? If the input was (B, T) indicies, then at every single (B, T) we calculate logits(vocab_size)\n",
    "        # ? for what token comes next in the sequence. Thus et every single token in the sequence\n",
    "        # ? we are predicting the next token in the sequence and this is done in parallel for all the tokens\n",
    "        # ? in the sequence.\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # (B*T, 65)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d09dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e7acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff8d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc571c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b46380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionHead(nn.Module):\n",
    "    \"\"\"One head of cross-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        B, T, C = x.shape\n",
    "        # k,v from encoder output, q from decoder input\n",
    "        k = self.key(encoder_output)    # (B,S,C) where S is encoder sequence length\n",
    "        q = self.query(x)               # (B,T,C)\n",
    "        v = self.value(encoder_output)  # (B,S,C)\n",
    "\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B,T,S)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # weighted aggregation of values\n",
    "        out = wei @ v  # (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of cross-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CrossAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        out = torch.cat([h(x, encoder_output) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Transformer decoder block: self-attention, cross-attention, and computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # self-attention\n",
    "        self.ca = MultiHeadCrossAttention(n_head, head_size)  # cross-attention\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        # self-attention with residual\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # cross-attention with residual\n",
    "        x = x + self.ca(self.ln2(x), encoder_output)\n",
    "        # feedforward with residual\n",
    "        x = x + self.ffwd(self.ln3(x))\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block: self-attention and computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Full Transformer with encoder and decoder\"\"\"\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size):\n",
    "        super().__init__()\n",
    "        # Encoder side\n",
    "        self.encoder_token_embedding = nn.Embedding(src_vocab_size, n_embd)\n",
    "        self.encoder_position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.encoder_ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Decoder side\n",
    "        self.decoder_token_embedding = nn.Embedding(tgt_vocab_size, n_embd)\n",
    "        self.decoder_position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.decoder_ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, tgt_vocab_size)\n",
    "\n",
    "    def encode(self, src_idx):\n",
    "        B, T = src_idx.shape\n",
    "        # Get embeddings\n",
    "        tok_emb = self.encoder_token_embedding(src_idx)\n",
    "        pos_emb = self.encoder_position_embedding(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through encoder blocks\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "        return self.encoder_ln_f(x)\n",
    "\n",
    "    def decode(self, tgt_idx, encoder_output):\n",
    "        B, T = tgt_idx.shape\n",
    "        # Get embeddings\n",
    "        tok_emb = self.decoder_token_embedding(tgt_idx)\n",
    "        pos_emb = self.decoder_position_embedding(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through decoder blocks\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, encoder_output)\n",
    "        x = self.decoder_ln_f(x)\n",
    "\n",
    "        # Get logits\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, src_idx, tgt_idx, targets=None):\n",
    "        # Encode source sequence\n",
    "        encoder_output = self.encode(src_idx)\n",
    "\n",
    "        # Decode target sequence\n",
    "        logits = self.decode(tgt_idx, encoder_output)\n",
    "\n",
    "        # If we have targets, compute loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, src_idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"Generate a target sequence given a source sequence\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # First encode the source sequence\n",
    "            encoder_output = self.encode(src_idx)\n",
    "\n",
    "            # Start with just a BOS token\n",
    "            B = src_idx.shape[0]\n",
    "            tgt_idx = torch.zeros((B, 1), dtype=torch.long, device=device)\n",
    "\n",
    "            # Generate one token at a time\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Get predictions for next token\n",
    "                logits = self.decode(tgt_idx, encoder_output)\n",
    "                logits = logits[:, -1, :] / temperature  # focus on last token\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample from the distribution\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "                # Append to the sequence\n",
    "                tgt_idx = torch.cat([tgt_idx, next_token], dim=1)\n",
    "\n",
    "        return tgt_idx\n",
    "\n",
    "# Example usage for translation task:\n",
    "# src_vocab_size = len(source_language_chars)\n",
    "# tgt_vocab_size = len(target_language_chars)\n",
    "# model = TransformerModel(src_vocab_size, tgt_vocab_size).to(device)\n",
    "#\n",
    "# # Training:\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# src_batch, tgt_batch = get_translation_batch('train')\n",
    "# logits, loss = model(src_batch, tgt_batch[:, :-1], tgt_batch[:, 1:])\n",
    "#\n",
    "# # Generation:\n",
    "# src_text = \"Hello\"  # example source text\n",
    "# src_ids = torch.tensor([encode_source(src_text)], device=device)\n",
    "# generated_ids = model.generate(src_ids, max_new_tokens=50)\n",
    "# generated_text = decode_target(generated_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a89ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder that processes the input sequence\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, n_layer, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Key difference from decoder: No causal mask in encoder attention\n",
    "        # Create encoder-specific attention block that allows bidirectional attention\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Get embeddings\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "\n",
    "        # Pass through encoder blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return self.ln_f(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder block with bidirectional attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.attention = EncoderMultiHeadAttention(n_head, head_size)\n",
    "        self.ff = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use pre-norm formulation like in the decoder\n",
    "        norm_x = self.ln1(x)\n",
    "        x = x + self.attention(norm_x)\n",
    "        norm_x = self.ln2(x)\n",
    "        x = x + self.ff(norm_x)\n",
    "        return x\n",
    "\n",
    "class EncoderMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention for encoder - allows bidirectional attention\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([EncoderHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class EncoderHead(nn.Module):\n",
    "    \"\"\"Single head of encoder self-attention - no causal mask\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Compute attention scores - no causal mask\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Decoder block with both self-attention and cross-attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        # Self attention (causal, like before)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        # Cross attention to encoder outputs\n",
    "        self.ca = CrossAttention(n_head, head_size)\n",
    "        # Feed forward\n",
    "        self.ff = FeedFoward(n_embd)\n",
    "        # Layer norms\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, encoder_out):\n",
    "        # Self attention\n",
    "        norm_x = self.ln1(x)\n",
    "        x = x + self.sa(norm_x)\n",
    "        # Cross attention\n",
    "        norm_x = self.ln2(x)\n",
    "        x = x + self.ca(norm_x, encoder_out)\n",
    "        # Feed forward\n",
    "        norm_x = self.ln3(x)\n",
    "        x = x + self.ff(norm_x)\n",
    "        return x\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention module to attend to encoder outputs\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CrossAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_out):\n",
    "        out = torch.cat([h(x, encoder_out) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class CrossAttentionHead(nn.Module):\n",
    "    \"\"\"Single head of cross-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_out):\n",
    "        # x is from decoder, encoder_out is from encoder\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Project encoder outputs to key and value\n",
    "        k = self.key(encoder_out)\n",
    "        v = self.value(encoder_out)\n",
    "        # Project decoder state to query\n",
    "        q = self.query(x)\n",
    "\n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class FullTransformer(nn.Module):\n",
    "    \"\"\"Complete Transformer with both encoder and decoder\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = TransformerEncoder(n_embd, n_head, n_layer//2)  # Use half the layers for encoder\n",
    "\n",
    "        # Decoder components\n",
    "        self.decoder_token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.decoder_position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            TransformerDecoderBlock(n_embd, n_head) for _ in range(n_layer//2)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, enc_idx, dec_idx, targets=None):\n",
    "        B, T = dec_idx.shape\n",
    "\n",
    "        # Run through encoder\n",
    "        encoder_out = self.encoder(enc_idx)\n",
    "\n",
    "        # Decoder embeddings\n",
    "        tok_emb = self.decoder_token_embedding(dec_idx)\n",
    "        pos_emb = self.decoder_position_embedding(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Run through decoder blocks\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, encoder_out)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, enc_idx, max_new_tokens, temperature=1.0):\n",
    "        # First encode the input sequence\n",
    "        encoder_out = self.encoder(enc_idx)\n",
    "\n",
    "        # Start with just the start token for decoder\n",
    "        dec_idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Auto-regressively generate tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if needed\n",
    "            dec_idx_cond = dec_idx[:, -block_size:]\n",
    "\n",
    "            # Forward pass through decoder\n",
    "            tok_emb = self.decoder_token_embedding(dec_idx_cond)\n",
    "            pos_emb = self.decoder_position_embedding(torch.arange(dec_idx_cond.size(1), device=device))\n",
    "            x = tok_emb + pos_emb\n",
    "\n",
    "            # Run through decoder blocks\n",
    "            for block in self.decoder_blocks:\n",
    "                x = block(x, encoder_out)\n",
    "\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            # Focus on last time step\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            dec_idx = torch.cat((dec_idx, idx_next), dim=1)\n",
    "\n",
    "        return dec_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL Learning (venv)",
   "language": "python",
   "name": "dl-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
