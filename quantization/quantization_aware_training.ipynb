{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Quantization Aware Training?\n",
    "\n",
    "When you quantize a model after training (Post-Training Quantization or PTQ), you replace float weights and activations with lower-precision (e.g. int8) versions after training is complete.\n",
    "But this can lead to significant accuracy drop, especially in sensitive models (like NLP or small models).\n",
    "\n",
    "To mitigate this, QAT simulates quantization during training, so the model learns to be robust to quantization artifacts.\n",
    "\n",
    "So we insert some fake modules in the computational graph of the model to simulate the effect of the quantization during\n",
    "training. This way, the loss function gets used to update weights that constantly suffer from the effect of quantization, and it usually leads to a more robust model.\n",
    "\n",
    "## üîß What Actually Happens in QAT?\n",
    "\n",
    "1. You start with a pretrained model in float32.\n",
    "\n",
    "2. You wrap layers with fake quantization modules like QuantStub, DeQuantStub, and layer-specific observers.\n",
    "\n",
    "3. During training:\n",
    "  - The model performs fake quantization on weights and activations.\n",
    "  - This means it simulates int8 rounding and clipping but keeps everything in float32.\n",
    "  - This ensures gradients can flow through and training is stable.\n",
    "  - The optimizer updates the weights to compensate for quantization errors.\n",
    "\n",
    "4. After training:\n",
    "  - The model is converted to real int8 quantized version.\n",
    "  - Now you have real quantized weights and can deploy to edge devices efficiently.\n",
    "\n",
    "## üîÅ The Quantize-Dequantize Flow in QAT\n",
    "The key thing QAT does is simulate this cycle:\n",
    "\n",
    "```\n",
    "Input (float32)\n",
    "   ‚Üì\n",
    "QuantStub  ‚ü∂  FakeQuantize (simulates int8)\n",
    "   ‚Üì\n",
    "Model layers (convolution, linear, etc.)\n",
    "   ‚Üì\n",
    "DeQuantStub ‚ü∂ back to float32 for output\n",
    "```\n",
    "\n",
    "Every layer internally also uses fake quantization to simulate quantized weights and activations.\n",
    "\n",
    "### In  Quantization Aware Training (QAT) all calculations are still in float32 during training:\n",
    "\n",
    "The fake quantization layers only simulate the effects of int8 by:\n",
    "\n",
    "1. Taking your float32 tensor\n",
    "\n",
    "2. Applying the quantization formula (round((x / scale) + zero_point))\n",
    "\n",
    "3. Clamping it to the int8 range (‚àí128 to 127 or 0 to 255)\n",
    "\n",
    "4. Then immediately converting it back to float32 for the rest of the computation.\n",
    "\n",
    "5. Here‚Äôs the flow inside the fake quantization modules:\n",
    "  - Quantize (fake):\n",
    "       1. Take your float32 activation\n",
    "       2. Simulate int8 mapping using scale & zero-point\n",
    "       3. Clamp to int8 range (like ‚àí128 to 127)\n",
    "       4. But store it still as float32\n",
    "  \n",
    "  - Dequantize (fake):\n",
    "       1. Convert that ‚Äúpretend int8‚Äù value back to float32 using scale * (q - zero_point)\n",
    "       2. Now the next layer still sees float32 input\n",
    "   \n",
    "  - Because of that:\n",
    "     1. You get all the precision loss of int8 (rounding + clipping)\n",
    "     2. But you can still train with normal float math\n",
    "\n",
    "6. So at every layer we have Sequence of Quantize and Dequantize operations:\n",
    "   ```\n",
    "   float32 ‚Üí fake quantize ‚Üí fake dequantize ‚Üí float32 ops\n",
    "   ```\n",
    "\n",
    "This way:\n",
    "- You get the rounding & clipping errors of int8 quantization.\n",
    "- But you still keep the full float32 computation graph so backpropagation and optimizer work normally.\n",
    "\n",
    "\n",
    "7. Key point:\n",
    "    - The real int8 math only happens after training for inference, when you run:\n",
    "\n",
    "```python\n",
    "model_int8 = torch.quantization.convert(model)\n",
    "```\n",
    "That‚Äôs when PyTorch swaps in actual quantized kernels and your weights & activations become int8 for real inference.\n",
    "The fake quantize + fake dequantize modules are replaced by real quantize ops\n",
    "The layers in between actually store and compute in int8.\n",
    "\n",
    "\n",
    "## ‚úÖ Benefits of QAT\n",
    "- Accuracy is typically much closer to the original float model.\n",
    "- Great for models that are sensitive to quantization errors.\n",
    "- Enables deployment to int8 hardware (e.g. mobile CPUs, edge devices).\n",
    "\n",
    "\n",
    "## Quantization Aware Training (QAT): gradient\n",
    "During backpropagation, the model needs to evaluate the gradient of the loss function w.r.t every weight and input. A problem arises:\n",
    "\n",
    "What is the derivative of the quantization operation we defined before?\n",
    "A typical solution is to approximate the gradient with the STE (Straight-through Estimator) approximation.\n",
    "The STE approximation results in 1 if the value being quantized is in the range [ùõº, ùõΩ], otherwise it is 0.\n",
    "(INTEGER QUANTIZATION FOR DEEP LEARNING INFERENCE: PRINCIPLES AND EMPIRICAL EVALUATION, Wu et al.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch deterministic\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the MNIST test set\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerySimpleNet(nn.Module):\n",
    "    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
    "        super(VerySimpleNet,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Step 2: Add QuantStub and DeQuantStub\n",
    "        # These are \"fake\" quant and dequant modules inserted into the model\n",
    "        # QuantStub: simulates quantizing activations at model input\n",
    "        # DeQuantStub: simulates dequantizing before output\n",
    "        # In QAT (Quantization-Aware Training):\n",
    "        # QuantStub and DeQuantStub actively simulate quantization during training using fake quantization ops.\n",
    "        # This means the forward pass mimics the rounding and clamping effects of int8 math using floating-point\n",
    "        # tensors, so the weights learn to be robust to the quantization noise.\n",
    "        # At the end of training, we still call convert(), which replaces the fake quant ops with real quantized\n",
    "        # int8 operators.\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)\n",
    "\n",
    "        # Quantize input\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        # Dequantize output\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "net = VerySimpleNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert min-max observers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VerySimpleNet(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.qconfig = torch.ao.quantization.default_qconfig\n",
    "net.train()\n",
    "\n",
    "\n",
    "# Prepare model for QAT\n",
    "# This inserts fake quant/dequant modules into the graph. These simulate the effect of real quantization.\n",
    "net_quantized = torch.ao.quantization.prepare_qat(net) # Insert observers\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:22<00:00, 271.31it/s, loss=0.224]\n"
     ]
    }
   ],
   "source": [
    "# Train (or fine-tune) the model with QAT\n",
    "# This is where the model learns to be robust to quantization effects.\n",
    "# Usually, you train for a few epochs only, on pre-trained weights.\n",
    "\n",
    "def train(train_loader, net, epochs=5, total_iterations_limit=None):\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x.view(-1, 28*28))\n",
    "            loss = cross_el(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
    "    os.remove('temp_delme.p')\n",
    "\n",
    "train(train_loader, net_quantized, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: nn.Module, total_iterations: int = None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x.view(-1, 784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "            iterations += 1\n",
    "            if total_iterations is not None and iterations >= total_iterations:\n",
    "                break\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the collected statistics during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VerySimpleNet(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.5494080781936646, max_val=0.3067437410354614)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-42.466209411621094, max_val=40.64482879638672)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.4645603895187378, max_val=0.33165353536605835)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-40.01139450073242, max_val=22.106660842895508)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.46996980905532837, max_val=0.21300984919071198)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-30.044422149658203, max_val=23.014163970947266)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers during training')\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize the model using the statistics collected during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the big differences between QAT and PTQ.\n",
    "\n",
    "- In PTQ:\n",
    "   - The model has never seen quantization effects during training\n",
    "   - You must run inference (with some representative dataset) to collect activation statistics (min/max or histogram)\n",
    "   - Those stats are used to calculate scale and zero-point for each quantization point\n",
    "   - Without that calibration step, you can‚Äôt convert the model because it wouldn‚Äôt know how to map float32 ‚Üí int8\n",
    "\n",
    "- In QAT:\n",
    "  - The model is trained with fake quantize/dequantize modules already in place\n",
    "  - Those modules collect stats during training itself\n",
    "  - By the end of training, they already know the right scale and zero-point\n",
    "  - So you can directly run torch.quantization.convert(model) without needing a separate calibration step and it‚Äôs ready   for int8 inference ‚Äî no extra calibration pass needed\n",
    "\n",
    "- So in short:\n",
    "  - PTQ ‚Üí needs post-training calibration\n",
    "  - QAT ‚Üí calibration is built into training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_quantized.eval()\n",
    "\n",
    "# Convert model to a fully quantized version\n",
    "# This replaces fake quant/dequant with real int8 arithmetic operations\n",
    "net_quantized = torch.ao.quantization.convert(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VerySimpleNet(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.6544176340103149, zero_point=65, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.4891185462474823, zero_point=82, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.4177841544151306, zero_point=72, qscheme=torch.per_tensor_affine)\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights and size of the model after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before quantization\n",
      "tensor([[ 4,  9, -3,  ...,  9,  5,  5],\n",
      "        [-7, -6, -5,  ..., -7, -4, -9],\n",
      "        [ 0,  8, -3,  ...,  0,  5,  6],\n",
      "        ...,\n",
      "        [ 8,  9,  1,  ...,  0,  4, -4],\n",
      "        [-2,  0,  8,  ...,  3,  3,  3],\n",
      "        [ 6,  4,  1,  ...,  9, -2,  3]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model before quantization\n",
    "print('Weights before quantization')\n",
    "print(torch.int_repr(net_quantized.linear1.weight()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model after quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 716.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "# Now this model uses real quantized weights and activations for inference\n",
    "print('Testing the model after quantization')\n",
    "test(net_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "- QuantStub/DeQuantStub simulate what real quantized ops would look like, so model can train while seeing quant noise.\n",
    "- Fake Quantization: forward pass quantizes and dequantizes activations/weights, backward is full precision.\n",
    "- QAT gives better accuracy than post-training quantization because it teaches the model to handle quantization artifacts.\n",
    "- Final Convert Step actually drops fake modules and replaces them with quantized implementations (e.g., int8 matmul)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
