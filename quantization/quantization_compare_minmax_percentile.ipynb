{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Quantization Range `[Î±, Î²]`\n",
    "\n",
    "If the vector V represents the tensor to be quantized, we can choose the [ð›¼, ð›½] range according to the following strategies:\n",
    "\n",
    "## 1. Min-Max Quantization\n",
    "\n",
    "- Set `Î± = min(V)` and `Î² = max(V)`\n",
    "- Covers the full range of values in `V`\n",
    "\n",
    "### Pros\n",
    "- Simple to implement\n",
    "\n",
    "### Cons\n",
    "- **Sensitive to outliers**\n",
    "- A single large outlier can stretch the range and reduce precision for the rest of the values\n",
    "\n",
    "### Example\n",
    "```\n",
    "Original Tensor:        43.31 -44.93 0 â€¦ 38.48 -20.49 1000.00 -28.02\n",
    "Dequantized (Min-Max):  45.08 -45.08 0 24.59 -45.08 -12.29 36.88 -20.49 999.85 -28.68\n",
    "```\n",
    "\n",
    "Note: The outlier (`1000`) is preserved, but most other values are quantized poorly.\n",
    "\n",
    "In the context of quantization\n",
    "When quantizing a tensor (like a weight matrix), choosing the quantization range [Î±, Î²] using min and max values means you're including all the values â€” even extreme outliers (very large or very small values that are rare).\n",
    "\n",
    "But these outliers can stretch the range unnecessarily, making the quantization less precise for the rest of the values.\n",
    "\n",
    "To fix this, we use percentile-based quantization:\n",
    "\n",
    "Percentile-based strategy\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Percentile-Based Quantization\n",
    "\n",
    "- Instead of using the min and max, choose a lower percentile (e.g., 0.1%) and upper percentile (e.g., 99.9%) of the data\n",
    "- Helps to ignore outliers while preserving meaningful value distribution\n",
    "\n",
    "Instead of \n",
    "```math\n",
    "Î± = min(V)\n",
    "\\\\\n",
    "\n",
    "Î² = max(V)\n",
    "```\n",
    "\n",
    "we set\n",
    "```math\n",
    "Î± = value at the 1st percentile of V\n",
    "\\\\\n",
    "\n",
    "Î² = value at the 99th percentile of V\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "We ignore the lowest 1% of the values (as outliers)\n",
    "We ignore the highest 1% of the values (as outliers)\n",
    "And we quantize only the middle 98% more tightly, giving better precision to most of the values\n",
    "\n",
    "\n",
    "### Pros\n",
    "- Less sensitive to extreme values\n",
    "- Preserves quality better for most values\n",
    "- Why use percentiles?\n",
    "   To avoid outliers dominating the quantization range\n",
    "   To reduce quantization error for the majority of the values\n",
    "   Useful when exact reconstruction of rare/extreme values isn't as important as accuracy for the common values\n",
    "\n",
    "### Example\n",
    "```\n",
    "Original Tensor:        43.31 -44.93 0 â€¦ 38.48 -20.49 1000.00 -28.02\n",
    "Dequantized (Percentile): 43.38 -44.52 0 â€¦ 38.48 -20.49 50.00 -28.01\n",
    "```\n",
    "\n",
    "Note: Outlier (`1000`) is now clipped, but the rest are better preserved.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Mean Squared Error (MSE) Optimized Range\n",
    "\n",
    "- Choose `Î±, Î²` that **minimize the mean squared error** between original values and dequantized values\n",
    "\n",
    "```math\n",
    "argmin_{Î±, Î²} \\sum_i (V_i - \\hat{V}_i)^2\n",
    "```\n",
    "\n",
    "- Typically solved using **grid search** over possible `[Î±, Î²]` pairs\n",
    "\n",
    "### Pros\n",
    "- Optimizes for the most accurate reconstruction of values\n",
    "\n",
    "### Cons\n",
    "- Computationally expensive\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Cross-Entropy Optimized Range (Used in LLMs)\n",
    "\n",
    "- Used when some values are more important than others (e.g., logits passed through Softmax in LLMs)\n",
    "- Goal: Preserve the **ordering**  as well as the distribution of values in `V` which is important for tasks like language modeling where the probability distribution matters.\n",
    "\n",
    "- Used when the values in the tensor being quantized are not equally important. This happens for example in the\n",
    "Softmax layer in Large Language Models. Since most of the inference strategies are Greedy, Top-P or Beam search, it is\n",
    "important to preserve the order of the largest values after quantization.\n",
    "\n",
    "```math\n",
    "argmin_{Î±, Î²} \\text{CrossEntropy}(softmax(V), softmax(\\hat{V}))\n",
    "```\n",
    "\n",
    "### Use Case\n",
    "- Softmax layers in LLMs where decoding relies on relative magnitude (Top-k, Top-p, Beam search)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Strategies\n",
    "\n",
    "| Strategy        | Goal                             | Strength                          | Weakness                      |\n",
    "|----------------|----------------------------------|-----------------------------------|-------------------------------|\n",
    "| Min-Max         | Full coverage of values          | Simple, covers entire range       | Sensitive to outliers         |\n",
    "| Percentile      | Focus on central distribution    | Robust to outliers                | Clips extreme values          |\n",
    "| MSE Optimized   | Best numerical reconstruction    | High accuracy                     | Requires search               |\n",
    "| Cross-Entropy   | Best ranking preservation        | Good for LLM inference            | Complex, needs task knowledge |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple tensor with random items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  96.79  -30.04  144.33 ...   24.16   12.02 1000.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Generate randomly distributed parameters\n",
    "params = np.random.uniform(low=-50, high=150, size=10000)\n",
    "\n",
    "# Introduce an outlier\n",
    "params[-1] = 1000\n",
    "\n",
    "# Round each number to the second decimal place\n",
    "params = np.round(params, 2)\n",
    "\n",
    "# Print the parameters\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the quantization methods and quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare min-max and percentile range selection strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "[  96.79  -30.04  144.33 ...   24.16   12.02 1000.  ]\n",
      "\n",
      "Asymmetric (min-max) scale: 4.117529411764706, zero: 12.0\n",
      "[ 36   5  47 ...  18  15 255]\n",
      "\n",
      "Asymmetric (percentile) scale: 0.7844509882329367, zero: 64.0\n",
      "[187  26 248 ...  95  79 255]\n"
     ]
    }
   ],
   "source": [
    "def clamp(params_q: np.array, lower_bound: int, upper_bound: int) -> np.array:\n",
    "    params_q[params_q < lower_bound] = lower_bound\n",
    "    params_q[params_q > upper_bound] = upper_bound\n",
    "    return params_q\n",
    "\n",
    "def asymmetric_quantization(params: np.array, bits: int) -> tuple[np.array, float, int]:\n",
    "    alpha = np.max(params)\n",
    "    beta = np.min(params)\n",
    "    scale = (alpha - beta) / (2**bits-1)\n",
    "    zero = -1*np.round(beta / scale)\n",
    "    lower_bound, upper_bound = 0, 2**bits-1\n",
    "    quantized = clamp(np.round(params / scale + zero), lower_bound, upper_bound).astype(np.int32)\n",
    "    return quantized, scale, zero\n",
    "\n",
    "def asymmetric_quantization_percentile(params: np.array, bits: int, percentile: float = 99.99) -> tuple[np.array, float, int]:\n",
    "    # find the percentile value\n",
    "    alpha = np.percentile(params, percentile)\n",
    "    beta = np.percentile(params, 100-percentile)\n",
    "    scale = (alpha - beta) / (2**bits-1)\n",
    "    zero = -1*np.round(beta / scale)\n",
    "    lower_bound, upper_bound = 0, 2**bits-1\n",
    "    quantized = clamp(np.round(params / scale + zero), lower_bound, upper_bound).astype(np.int32)\n",
    "    return quantized, scale, zero\n",
    "\n",
    "\n",
    "def asymmetric_dequantize(params_q: np.array, scale: float, zero: int) -> np.array:\n",
    "    return (params_q - zero) * scale\n",
    "\n",
    "def quantization_error(params: np.array, params_q: np.array):\n",
    "    # calculate the MSE\n",
    "    return np.mean((params - params_q)**2)\n",
    "\n",
    "(asymmetric_q, asymmetric_scale, asymmetric_zero) = asymmetric_quantization(params, 8)\n",
    "(asymmetric_q_percentile, asymmetric_scale_percentile, asymmetric_zero_percentile) = asymmetric_quantization_percentile(params, 8)\n",
    "\n",
    "print(f'Original:')\n",
    "print(np.round(params, 2))\n",
    "print('')\n",
    "print(f'Asymmetric (min-max) scale: {asymmetric_scale}, zero: {asymmetric_zero}')\n",
    "print(asymmetric_q)\n",
    "print(f'')\n",
    "print(f'Asymmetric (percentile) scale: {asymmetric_scale_percentile}, zero: {asymmetric_zero_percentile}')\n",
    "print(asymmetric_q_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "[  96.79  -30.04  144.33 ...   24.16   12.02 1000.  ]\n",
      "\n",
      "Dequantized (min-max):\n",
      "[  98.82  -28.82  144.11 ...   24.71   12.35 1000.56]\n",
      "\n",
      "Dequantized (percentile):\n",
      "[ 96.49 -29.81 144.34 ...  24.32  11.77 149.83]\n"
     ]
    }
   ],
   "source": [
    "# Dequantize the parameters back to 32 bits\n",
    "params_deq_asymmetric = asymmetric_dequantize(asymmetric_q, asymmetric_scale, asymmetric_zero)\n",
    "params_deq_asymmetric_percentile = asymmetric_dequantize(asymmetric_q_percentile, asymmetric_scale_percentile, asymmetric_zero_percentile)\n",
    "\n",
    "print(f'Original:')\n",
    "print(np.round(params, 2))\n",
    "print('')\n",
    "print(f'Dequantized (min-max):')\n",
    "print(np.round(params_deq_asymmetric,2))\n",
    "print('')\n",
    "print(f'Dequantized (percentile):')\n",
    "print(np.round(params_deq_asymmetric_percentile,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the quantization error (excluding the outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Error (min-max) excluding outlier: 1.39\n",
      "  Error (percentile) excluding outlier: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Calculate the quantization error\n",
    "print(f'{\"Error (min-max) excluding outlier: \":>40}{np.round(quantization_error(params[:-1], params_deq_asymmetric[:-1]),2)}')\n",
    "print(f'{\"Error (percentile) excluding outlier: \":>40}{np.round(quantization_error(params[:-1], params_deq_asymmetric_percentile[:-1]),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probability-statistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
