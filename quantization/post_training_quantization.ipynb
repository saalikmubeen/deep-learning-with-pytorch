{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Training Quantization (PTQ)\n",
    "\n",
    "\n",
    "\n",
    "## Step-by-Step PTQ Flow:\n",
    "\n",
    "1. Start with a Pretrained Full-Precision Model (float32):\n",
    "   You start with a trained model (say in PyTorch or TensorFlow), with weights and activations in float32.\n",
    "\n",
    "2. Attach observers to the model (usually to each layer or submodule)\n",
    "   Observers track min/max or percentile values of activations during inference\n",
    "\n",
    "3. Run calibration data (unlabeled):\n",
    "    Attach observers to the model (usually to each layer or submodule)\n",
    "    To prepare for quantization, you do:\n",
    "      - Attach observers to layers. These track the range of values seen during a forward pass (min/max or percentiles).\n",
    "      - Run calibration data (a few batches of real input data) to collect the stats.\n",
    "     This step doesn't modify the model yet, it just gathers info about the distribution of values in weights and especially activations. Why? Because quantization requires knowing the range of values to scale them properly.\n",
    "\n",
    "     What About Outputs Like Y = XW + B?\n",
    "\n",
    "     When we do inference:\n",
    "     Suppose X and W are quantized. Then Y = XW + B is done in int32.But Y is now a new tensor we haven’t seen before\n",
    "     So... how do we dequantize Y?\n",
    "     Answer: we observe Y during calibration too. You pass a few real inputs through the model and record Y’s range or percentiles → compute its scale and zero-point too.\n",
    "     Observers track min/max or percentile values of activations during inference\n",
    "\n",
    "\n",
    "4. Compute scale (s) and zero-point (z) using:\n",
    "   Min-max or Percentile ranges to avoid outliers\n",
    "\n",
    "5. Quantize weights and activations to int8 using computed s and z\n",
    "\n",
    "6. Perform inference using integer-only arithmetic\n",
    "\n",
    "###  Calibration and Observers (Key Insight)\n",
    "\n",
    "You can’t quantize well unless you know the range of values (min, max, or distribution).\n",
    "So, we run a few batches of real (but unlabeled) data through the model and record:\n",
    "Min/max values of weights and activations Or better: percentiles (to ignore extreme outliers)\n",
    "\n",
    "These recorded values are then used to compute the scale and zero-point for each layer.\n",
    "\n",
    "### Input sometimes called “activation” can be quantized “on the fly” using a process called (dynamic quantization) or with observers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Post-Training Quantization: Weights & Biases Only (No Activations)\n",
    "\n",
    "## Goal\n",
    "Convert the trained model's float32 weights and biases to int8 (or lower-bit) integers to reduce memory usage and improve inference efficiency, especially on edge devices.\n",
    "\n",
    "---\n",
    "\n",
    "## Starting Point\n",
    "You have a fully trained model with:\n",
    "- Weights `W` (float32)\n",
    "- Biases `B` (float32)\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Flow (for Weights & Biases Only)\n",
    "\n",
    "### Step 1: Load the Trained Model\n",
    "- The model is already trained with float32 weights and biases.\n",
    "\n",
    "### Step 2: Attach **Observers**\n",
    "- Observers are modules that **record the min and max** of the weights.\n",
    "- This range helps us decide how to map float values to integers.\n",
    "\n",
    "### Step 3: **Calibrate the Weights**\n",
    "- Calibration = Run the model or feed dummy data through it **just to let observers collect statistics.**\n",
    "- You **don’t need labels**.\n",
    "- This is **not training**; it’s just **observation**.\n",
    "- From the observed min/max range of weights, we compute two parameters:\n",
    "  - `s` = scale\n",
    "  - `z` = zero-point\n",
    "\n",
    "### Step 4: **Quantize the Weights and Biases**\n",
    "- Use the computed `(s, z)` from calibration to convert:\n",
    "  - `W_fp32 → W_int8`\n",
    "  - `B_fp32 → B_int32` *(usually higher precision for bias)*\n",
    "- Now the model stores quantized values.\n",
    "- Statically after calibration (static quantization)\n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- **We don’t dequantize W or B during inference.**\n",
    "  - Once quantized, they stay in int8 and are directly used in int8 operations.\n",
    "- **No backpropagation here.**\n",
    "  - This is for inference only. Weights are \"frozen\".\n",
    "- **Bias is usually quantized more precisely** (e.g., int32) to preserve numerical accuracy during matrix multiplications.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits\n",
    "- Smaller model size (int8 is 4x smaller than float32).\n",
    "- Faster inference on CPUs or low-power hardware (e.g., phones, microcontrollers).\n",
    "\n",
    "\n",
    "# Post-Training Quantization: Activations (e.g., ReLU outputs)\n",
    "\n",
    "## 1. Pretrained Model\n",
    "- You start with a fully trained model with:\n",
    "  - Float32 weights `W`\n",
    "  - Float32 biases `B`\n",
    "  - Float32 activations `X`, `Y`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Attach **Activation Observers**\n",
    "- Insert observer modules **after each activation** or output of interest.\n",
    "  - For example, after `ReLU`, `GELU`, etc.\n",
    "- These observers will record the **min and max** values of the outputs during real inference-like usage.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Calibrate the Activations**\n",
    "- Run inference on some **representative data** (a few hundred to thousand samples).\n",
    "  - This lets the observers **collect the actual min/max** ranges of activation outputs.\n",
    "  - No gradients, no labels needed.\n",
    "- From this range, compute:\n",
    "  - `s_act` = activation scale\n",
    "  - `z_act` = activation zero-point\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Quantize Activations at Runtime\n",
    "- You **do not statically quantize** activations like weights.\n",
    "- Instead:\n",
    "  - At inference time, input tensors (e.g., `X`) are quantized **on the fly** using the `(scale, zero_point)` learned during calibration:\n",
    "    ```\n",
    "    X_int8 = round(X_fp32 / s_act + z_act)\n",
    "    ```\n",
    "  - Then integer matrix multiplication is performed:\n",
    "    ```\n",
    "    Y_int32 = W_int8 * X_int8 + B_int32\n",
    "    ```\n",
    "  - And the output is **requantized** to int8 using the output scale and zero point:\n",
    "    ```\n",
    "    Y_int8 = round(Y_int32 * s_output) + z_output\n",
    "    ```\n",
    "  - Dynamically at runtime (dynamic quantization)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Inference (Quantized Model)\n",
    "- Now all forward passes use:\n",
    "  - `int8` inputs\n",
    "  - `int8` weights\n",
    "  - `int32` accumulations\n",
    "  - `int8` outputs\n",
    "\n",
    "- Float32 only appears if you convert inputs or outputs back to float.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Component       | Quantized? | How Quantized             | Static or Dynamic |\n",
    "|----------------|------------|---------------------------|-------------------|\n",
    "| Weights         | ✅ Yes     | After calibration          | Static            |\n",
    "| Biases          | ✅ Yes     | Derived from weights & act | Static (int32)    |\n",
    "| Activations     | ✅ Yes     | Calibrated using observers | Dynamic at runtime|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch deterministic\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the MNIST test set\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerySimpleNet(nn.Module):\n",
    "    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
    "        super(VerySimpleNet,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VerySimpleNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, net, epochs=5, total_iterations_limit=None):\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x.view(-1, 28*28))\n",
    "            loss = cross_el(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
    "    os.remove('temp_delme.p')\n",
    "\n",
    "MODEL_FILENAME = 'simplenet_ptq.pt'\n",
    "\n",
    "if Path(MODEL_FILENAME).exists():\n",
    "    net.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    print('Loaded model from disk')\n",
    "else:\n",
    "    train(train_loader, net, epochs=1)\n",
    "    # Save the model to disk\n",
    "    torch.save(net.state_dict(), MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: nn.Module, total_iterations: int = None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x.view(-1, 784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "            iterations += 1\n",
    "            if total_iterations is not None and iterations >= total_iterations:\n",
    "                break\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights and size of the model before quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before quantization\n",
      "Parameter containing:\n",
      "tensor([[-0.0068,  0.0126, -0.0359,  ...,  0.0154, -0.0028, -0.0045],\n",
      "        [-0.0141, -0.0093, -0.0048,  ..., -0.0146, -0.0003, -0.0243],\n",
      "        [ 0.0251,  0.0601,  0.0120,  ...,  0.0249,  0.0464,  0.0533],\n",
      "        ...,\n",
      "        [ 0.0564,  0.0601,  0.0255,  ...,  0.0201,  0.0394,  0.0024],\n",
      "        [-0.0070,  0.0011,  0.0332,  ...,  0.0135,  0.0135,  0.0130],\n",
      "        [ 0.0103,  0.0049, -0.0092,  ...,  0.0272, -0.0221, -0.0020]],\n",
      "       requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model before quantization\n",
    "print('Weights before quantization')\n",
    "print(net.linear1.weight)\n",
    "print(net.linear1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model before quantization\n",
      "Size (KB): 360.559\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model before quantization')\n",
    "print_size_of_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model before quantization: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  15%|█▌        | 150/1000 [00:00<00:01, 709.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 799.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of the model before quantization: ')\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert min-max observers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedVerySimpleNet(nn.Module):\n",
    "    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
    "        super(QuantizedVerySimpleNet,self).__init__()\n",
    "\n",
    "        # torch.quantization.QuantStub()\n",
    "        # In PTQ (Post-Training Quantization):\n",
    "        # QuantStub and DeQuantStub are placeholders in the model.\n",
    "        # During model preparation, they don't actually quantize or dequantize the data yet — they just mark\n",
    "        # the spots in the model graph where quantization and dequantization should happen once we convert the model.\n",
    "        # After calibration and conversion, these are replaced by real quant/dequant ops that perform integer math.\n",
    "        # So yes, they're \"fake\" initially, but become real ops after convert()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you place a QuantStub() in the forward pass, everything between that quantize() call and the matching dequantize() call will run in the quantized domain after convert() — as long as the layers inside are quantizable modules (like nn.Linear, nn.Conv2d, etc.) and have been prepared with observers.\n",
    "\n",
    "```\n",
    "float32 input \n",
    "   │\n",
    "QuantStub → (convert) → real quantize (float32 → int8)\n",
    "   │\n",
    "[ int8 layer1 → int8 layer2 → int8 layer3 ... ]\n",
    "   │\n",
    "DeQuantStub → (convert) → real dequantize (int8 → float32)\n",
    "   │\n",
    "float32 output\n",
    "```\n",
    "\n",
    "You don’t have to sprinkle self.quant() before every layer — the quantization state is sticky until you explicitly dequantize.\n",
    "\n",
    "Important details:\n",
    "  - During prepare, PyTorch automatically attaches per-tensor/per-channel observers to all eligible layers between those stubs.\n",
    "  - At convert, those float modules get swapped for quantized equivalents (nnq.Linear, nnq.Conv2d) that expect int8 inputs and store int8 weights.\n",
    "  - Any layer outside the quant-dequant block stays float.\n",
    "\n",
    "`torch.quantization.QuantStub()` is a module in PyTorch's `torch.ao.quantization` library used for quantization. It acts as a placeholder or \"stub\" within a neural network model, indicating where a floating-point tensor should be quantized to a lower-precision format (e.g., 8-bit integer) during the quantization process.\n",
    "\n",
    "Here's a breakdown of its role:\n",
    "\n",
    "- Quantization Point: QuantStub explicitly marks the input to a section of the model that will be quantized. When performing post-training static quantization or quantization-aware training, PyTorch's quantization tools identify these QuantStub instances and insert the necessary quantization operations (like observers and quantizers) during the prepare and convert stages.\n",
    "\n",
    "- Calibration and Conversion: During calibration, QuantStub (along with a qconfig set on the model) helps collect statistics about the activation distributions to determine appropriate quantization parameters (scale and zero-point). In the convert step, QuantStub instances are replaced by actual torch.nn.quantized.Quantize modules, which perform the conversion from floating-point to quantized tensors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### You dequantize the model outputs before returning them.\n",
    "\n",
    "Here’s why:\n",
    "  -Quantized tensors are usually in torch.qint8 or torch.quint8 format, which most downstream code (loss functions, evaluation metrics, plotting, saving to JSON, etc.) doesn’t understand.\n",
    "  Dequantizing converts them back to standard torch.float32 values so they can be consumed normally.\n",
    "\n",
    "```\n",
    "float input  \n",
    "   │\n",
    "QuantStub  (float → int8)\n",
    "   │\n",
    "[int8 layers ... int8 weights, int8 activations]\n",
    "   │\n",
    "DeQuantStub  (int8 → float)\n",
    "   │\n",
    "float output\n",
    "```\n",
    "\n",
    "The final output of your model (before you hand it back to the user or another float-based component) should usually be dequantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimpleNet(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_quantized = QuantizedVerySimpleNet().to(device)\n",
    "# Copy weights from unquantized model\n",
    "net_quantized.load_state_dict(net.state_dict())\n",
    "net_quantized.eval()\n",
    "\n",
    "net_quantized.qconfig = torch.ao.quantization.default_qconfig\n",
    "# Attach a quantization configuration (qconfig) to model\n",
    "# This tells PyTorch which observer to use (histogram, min-max, etc.)\n",
    "# \"fbgemm\" is backend optimized for x86 CPUs\n",
    "# net_quantized.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "\n",
    "#  This inserts observers in the model layers (for both weights and activations)\n",
    "# Observers record min/max ranges during calibration to later compute scale (s) and zero point (z)\n",
    "net_quantized = torch.ao.quantization.prepare(net_quantized) # Insert observers\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrate the model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 750.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calibration - pass real or synthetic data through the model\n",
    "# This will \"observe\" the weights (already known) and activations (depends on data)\n",
    "# It does NOT update weights; just gathers activation stats via observers\n",
    "test(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimpleNet(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-53.58397674560547, max_val=34.898128509521484)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-24.331275939941406, max_val=26.62542152404785)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-28.273700714111328, max_val=20.937761306762695)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize the model using the statistics collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This step:\n",
    "#   - Quantizes weights using s and z computed from calibration\n",
    "#   - Quantizes activations using the observed ranges\n",
    "#   - Replaces float ops with quantized integer equivalents\n",
    "net_quantized = torch.ao.quantization.convert(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimpleNet(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.6967094540596008, zero_point=77, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.40123382210731506, zero_point=61, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.3874918520450592, zero_point=73, qscheme=torch.per_tensor_affine)\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "net_quantized\n",
    "\n",
    "# At this point, the model is fully quantized: weights (W), biases (B), and activations\n",
    "# All matmuls, relus, etc. now use INT8 arithmetic internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights of the model after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after quantization\n",
      "tensor([[-2,  3, -8,  ...,  4, -1, -1],\n",
      "        [-3, -2, -1,  ..., -3,  0, -6],\n",
      "        [ 6, 14,  3,  ...,  6, 11, 12],\n",
      "        ...,\n",
      "        [13, 14,  6,  ...,  5,  9,  1],\n",
      "        [-2,  0,  8,  ...,  3,  3,  3],\n",
      "        [ 2,  1, -2,  ...,  6, -5,  0]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model after quantization\n",
    "print('Weights after quantization')\n",
    "print(torch.int_repr(net_quantized.linear1.weight()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the dequantized weights and the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights: \n",
      "Parameter containing:\n",
      "tensor([[-0.0068,  0.0126, -0.0359,  ...,  0.0154, -0.0028, -0.0045],\n",
      "        [-0.0141, -0.0093, -0.0048,  ..., -0.0146, -0.0003, -0.0243],\n",
      "        [ 0.0251,  0.0601,  0.0120,  ...,  0.0249,  0.0464,  0.0533],\n",
      "        ...,\n",
      "        [ 0.0564,  0.0601,  0.0255,  ...,  0.0201,  0.0394,  0.0024],\n",
      "        [-0.0070,  0.0011,  0.0332,  ...,  0.0135,  0.0135,  0.0130],\n",
      "        [ 0.0103,  0.0049, -0.0092,  ...,  0.0272, -0.0221, -0.0020]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Dequantized weights: \n",
      "tensor([[-0.0087,  0.0131, -0.0348,  ...,  0.0174, -0.0044, -0.0044],\n",
      "        [-0.0131, -0.0087, -0.0044,  ..., -0.0131,  0.0000, -0.0261],\n",
      "        [ 0.0261,  0.0609,  0.0131,  ...,  0.0261,  0.0479,  0.0522],\n",
      "        ...,\n",
      "        [ 0.0566,  0.0609,  0.0261,  ...,  0.0218,  0.0392,  0.0044],\n",
      "        [-0.0087,  0.0000,  0.0348,  ...,  0.0131,  0.0131,  0.0131],\n",
      "        [ 0.0087,  0.0044, -0.0087,  ...,  0.0261, -0.0218,  0.0000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original weights: ')\n",
    "print(net.linear1.weight)\n",
    "print('')\n",
    "print(f'Dequantized weights: ')\n",
    "print(torch.dequantize(net_quantized.linear1.weight()))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print size and accuracy of the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model after quantization\n",
      "Size (KB): 94.955\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model after quantization')\n",
    "print_size_of_model(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model after quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 774.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model after quantization')\n",
    "test(net_quantized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
